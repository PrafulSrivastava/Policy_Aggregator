# Story 2.2: HTML Source Fetcher Implementation

## Status
Draft

## Story
**As a** developer,  
**I want** HTML scraping capability using Requests and BeautifulSoup,  
**so that** I can fetch policy content from government websites.

## Acceptance Criteria

1. HTML fetcher module that uses `requests` library to fetch web pages
2. BeautifulSoup integration for parsing HTML and extracting text content
3. Handles common HTML structures: main content areas, article tags, div containers
4. Strips HTML tags and returns clean text content
5. Handles HTTP errors gracefully (404, 500, timeout) with retry logic
6. Respects `robots.txt` (checks before fetching, logs if blocked)
7. User-Agent header configured appropriately
8. Handles redirects correctly
9. Extracts and preserves important metadata: page title, last modified date (if available)
10. Unit tests with mock HTTP responses
11. At least one working HTML fetcher for a real Germany immigration source (test source)

## Tasks / Subtasks

- [x] Task 1: Create base HTML fetcher module (AC: 1, 2, 3, 4)
  - [x] Create `fetchers/html_fetcher.py` or base HTML fetcher class
  - [x] Implement `fetch(url: str, metadata: dict) -> FetchResult` function
  - [x] Use `requests` library to fetch web page
  - [x] Use BeautifulSoup to parse HTML
  - [x] Extract text from common HTML structures:
    - [x] `<main>` tags
    - [x] `<article>` tags
    - [x] `<div>` containers with content classes
    - [x] Fallback to `<body>` if no specific content area found
  - [x] Strip HTML tags and return clean text
  - [x] Preserve basic structure (paragraphs, line breaks)
- [x] Task 2: Add HTTP error handling (AC: 5)
  - [x] Handle HTTP status codes:
    - [x] 404 Not Found
    - [x] 500 Internal Server Error
    - [x] Other 4xx/5xx errors
  - [x] Implement retry logic:
    - [x] Retry transient failures (500, 502, 503, 504) 2-3 times
    - [x] Use exponential backoff between retries
    - [x] Handle timeout errors with retry
  - [x] Return error status in `FetchResult` on failure
  - [x] Log errors appropriately
- [x] Task 3: Implement robots.txt checking (AC: 6)
  - [x] Install `urllib.robotparser` or `robotparser` library
  - [x] Before fetching, check `robots.txt` for URL
  - [x] If blocked, log warning and return error in `FetchResult`
  - [x] If allowed, proceed with fetch
  - [x] Handle robots.txt fetch errors gracefully (proceed if robots.txt unavailable)
- [x] Task 4: Configure HTTP headers (AC: 7, 8)
  - [x] Set User-Agent header:
    - [x] Use descriptive user agent (e.g., "PolicyAggregator/1.0")
    - [x] Include contact information if required
  - [x] Handle redirects:
    - [x] Allow redirects (default `requests` behavior)
    - [x] Follow redirects up to reasonable limit (e.g., 5 redirects)
    - [x] Preserve final URL in metadata
- [x] Task 5: Extract metadata from HTML (AC: 9)
  - [x] Extract page title from `<title>` tag
  - [x] Extract last modified date if available:
    - [x] Check `<meta name="last-modified">` tag
    - [x] Check HTTP `Last-Modified` header
    - [x] Check other common date patterns
  - [x] Store metadata in `FetchResult.metadata` dictionary
- [x] Task 6: Create example HTML fetcher (AC: 11)
  - [x] Create `fetchers/de_bmi_student.py` (or similar Germany source)
  - [x] Implement `fetch()` function using HTML fetcher utilities
  - [x] Test with real source URL
  - [x] Verify content extraction works correctly
  - [x] Document source URL and what it monitors
- [x] Task 7: Create unit tests (AC: 10)
  - [x] Create `tests/unit/test_fetchers/test_html_fetcher.py`
  - [x] Test with mock HTTP responses:
    - [x] Successful fetch with HTML content
    - [x] 404 Not Found error
    - [x] 500 Internal Server Error
    - [x] Timeout error
  - [x] Test retry logic:
    - [x] Verify retries on transient failures
    - [x] Verify exponential backoff
  - [x] Test HTML parsing:
    - [x] Extract text from `<main>` tag
    - [x] Extract text from `<article>` tag
    - [x] Fallback to `<body>` if no content area
  - [x] Test metadata extraction:
    - [x] Page title extraction
    - [x] Last modified date extraction
  - [x] Test robots.txt checking:
    - [x] Blocked by robots.txt
    - [x] Allowed by robots.txt
    - [x] robots.txt unavailable

## Dev Notes

### Previous Story Insights
- Story 2.1 created fetcher plugin architecture and base interface
- `FetchResult` dataclass/model is defined
- Fetcher registry system is available
- Fetcher manager can load and execute fetchers

### Components Architecture
[Source: architecture/components.md]

**Source Fetchers (Plugin System):**
- Individual plugin modules for specific government sources
- Standard interface: `fetch(url, metadata) -> FetchResult`
- Technology: `requests` library for HTTP, `beautifulsoup4` for HTML parsing

**Dependencies:**
- External government sources (HTTP requests)
- `requests` library for HTTP
- `beautifulsoup4` for HTML parsing

### File Locations
[Source: architecture/unified-project-structure.md]

Files to create:
- `fetchers/html_fetcher.py` - Base HTML fetcher utilities (optional, can be in individual fetchers)
- `fetchers/de_bmi_student.py` - Example HTML fetcher for Germany BMI Student visa
- `tests/unit/test_fetchers/test_html_fetcher.py` - Unit tests

### Coding Standards
[Source: architecture/coding-standards.md]

**Fetcher Interface:** All source fetchers must implement the `SourceFetcher` interface. Never bypass the fetcher manager.

**Error Handling:** Fetchers should return error status in `FetchResult`, not raise exceptions.

**Logging:** Use Python's `logging` module for fetcher operations.

**Type Hints:** All Python functions must have type hints.

### Tech Stack
[Source: architecture/tech-stack.md]

**Dependencies to add:**
- `requests` - HTTP library
- `beautifulsoup4` - HTML parsing
- `lxml` or `html.parser` - BeautifulSoup parser backend

### Testing Requirements
[Source: architecture/testing-strategy.md]

**Unit Tests:**
- Test with mock HTTP responses using `responses` library or `unittest.mock`
- Test HTML parsing with sample HTML
- Test error handling scenarios
- Test retry logic

### Technical Constraints
- Must use `requests` library for HTTP
- Must use BeautifulSoup for HTML parsing
- Must respect robots.txt (check before fetching)
- Must handle HTTP errors gracefully
- Must implement retry logic for transient failures
- User-Agent must be configured appropriately

## Testing

### Testing Standards
[Source: architecture/testing-strategy.md]

**Test File Location:** `tests/unit/test_fetchers/test_html_fetcher.py`

**Test Standards:**
- Use pytest framework
- Use `responses` library or `unittest.mock` for HTTP mocking
- Test files follow `test_*.py` naming convention

**Testing Frameworks:**
- pytest 7.4+ with async support
- `responses` library for mocking HTTP requests (or `unittest.mock`)

**Specific Testing Requirements:**
- Test successful HTML fetch and parsing
- Test HTTP error handling (404, 500, timeout)
- Test retry logic with exponential backoff
- Test robots.txt checking (blocked, allowed, unavailable)
- Test HTML structure extraction (main, article, body)
- Test metadata extraction (title, last modified)
- Test redirect handling
- Test User-Agent header configuration

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-27 | 1.0 | Initial story creation | Scrum Master |

## Dev Agent Record

### Agent Model Used
Claude Sonnet 4.5 (Auto)

### Debug Log References
None - No debug issues encountered during implementation.

### Completion Notes List
- Created comprehensive HTML fetcher utilities module (`fetchers/html_fetcher.py`) with all required functionality
- Implemented `fetch_html()` function that handles robots.txt checking, HTTP requests with retry logic, HTML parsing, and metadata extraction
- Retry logic uses exponential backoff for transient failures (500, 502, 503, 504) with configurable max retries (default 3)
- Robots.txt checking implemented using `urllib.robotparser`, gracefully handles unavailable robots.txt files
- HTML text extraction prioritizes semantic elements: `<main>` → `<article>` → content divs → `<body>` fallback
- Metadata extraction includes: page title, last modified (from headers and meta tags), redirects, description
- User-Agent header configured with "PolicyAggregator/1.0" default, customizable via metadata
- Redirect handling with max 5 redirects, final URL preserved in metadata
- Created example fetcher `de_bmi_student.py` demonstrating usage of HTML fetcher utilities
- Comprehensive unit tests covering all functionality: HTML parsing, metadata extraction, error handling, retry logic, robots.txt checking
- Added `requests` and `responses` libraries to requirements.txt

### File List
**Created:**
- `fetchers/html_fetcher.py` - Comprehensive HTML fetcher utilities module
- `fetchers/de_bmi_student.py` - Example HTML fetcher for Germany BMI Student visa
- `tests/unit/test_fetchers/__init__.py` - Test fetchers package init
- `tests/unit/test_fetchers/test_html_fetcher.py` - Comprehensive unit tests

**Modified:**
- `requirements.txt` - Added `requests>=2.31.0` and `responses>=0.24.0` for HTTP and testing

## QA Results
_To be populated by QA Agent_

