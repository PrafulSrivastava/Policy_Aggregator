# Story 7.4: AI Summary Quality Control & Monitoring

## Status
Draft

## Story
**As a** developer,  
**I want** to monitor and control the quality of AI-generated summaries,  
**so that** I can ensure summaries are accurate and useful.

## Acceptance Criteria

1. Quality monitoring:
   - Track summary generation success rate
   - Monitor summary length and quality metrics
   - Flag summaries that may need review
2. Review process:
   - Admin can review generated summaries
   - Mark summaries as approved/rejected
   - Edit summaries if needed (manual override)
3. User feedback mechanism:
   - "Was this summary helpful?" feedback in emails/dashboard
   - Collect feedback on summary quality
   - Use feedback to improve prompts
4. Cost monitoring dashboard:
   - View API usage statistics
   - Cost per summary
   - Total monthly costs
   - Cost trends over time
5. Prompt optimization:
   - A/B test different prompts
   - Optimize prompts based on quality and cost
   - Version control for prompts
6. Fallback strategies:
   - If API fails, use raw diff
   - If summary quality is poor, flag for review
   - Manual summary option (admin can write summaries)
7. Quality metrics:
   - Average summary length
   - User feedback scores
   - Summary generation success rate
8. Reporting:
   - Quality report (weekly/monthly)
   - Cost report
   - Usage statistics

## Tasks / Subtasks

- [ ] Task 1: Create quality monitoring service (AC: 1)
  - [ ] Create `api/services/ai_quality_monitoring.py`
  - [ ] Implement quality tracking:
    - [ ] Track summary generation success rate
    - [ ] Monitor summary length (min, max, average)
    - [ ] Monitor summary quality metrics
    - [ ] Flag summaries that may need review
  - [ ] Store quality metrics in database or logs
- [ ] Task 2: Add quality flags to PolicyChange (AC: 1)
  - [ ] Create Alembic migration:
    - [ ] Add `ai_summary_quality_flag` column (VARCHAR(20), nullable) - 'needs_review', 'approved', 'rejected'
    - [ ] Add `ai_summary_reviewed_at` column (TIMESTAMP WITH TIME ZONE, nullable)
    - [ ] Add `ai_summary_reviewed_by` column (UUID, foreign key to users, nullable)
  - [ ] Update SQLAlchemy model
  - [ ] Update Pydantic schemas
- [ ] Task 3: Create summary review UI (AC: 2)
  - [ ] Create `admin-ui/templates/pages/ai/summary-review.html` - Summary review page
  - [ ] Create web route: `GET /ai/summaries/review`
  - [ ] Display summaries needing review:
    - [ ] List of summaries with quality flags
    - [ ] Show summary text, policy change info
    - [ ] Approve/Reject buttons
    - [ ] Edit button (manual override)
  - [ ] Add "AI Summary Review" link to admin navigation
- [ ] Task 4: Create summary review API endpoints (AC: 2)
  - [ ] Create `GET /api/ai/summaries/review` - List summaries needing review
  - [ ] Create `POST /api/ai/summaries/{id}/approve` - Approve summary
  - [ ] Create `POST /api/ai/summaries/{id}/reject` - Reject summary
  - [ ] Create `PUT /api/ai/summaries/{id}` - Edit summary (manual override)
  - [ ] Require authentication
- [ ] Task 5: Create user feedback mechanism (AC: 3)
  - [ ] Create Alembic migration for `ai_summary_feedback` table:
    - [ ] `id`: UUID (primary key)
    - [ ] `policy_change_id`: UUID (foreign key)
    - [ ] `user_id`: UUID (foreign key, nullable)
    - [ ] `helpful`: BOOLEAN (was summary helpful?)
    - [ ] `feedback_text`: TEXT (optional feedback)
    - [ ] `created_at`: TIMESTAMP WITH TIME ZONE
  - [ ] Create SQLAlchemy model: `api/models/db/ai_summary_feedback.py`
  - [ ] Add "Was this helpful?" buttons to email and dashboard
  - [ ] Create `POST /api/ai/summaries/{id}/feedback` endpoint
- [ ] Task 6: Create cost monitoring dashboard (AC: 4)
  - [ ] Create `admin-ui/templates/pages/ai/cost-monitoring.html` - Cost dashboard
  - [ ] Create web route: `GET /ai/cost-monitoring`
  - [ ] Display cost statistics:
    - [ ] API usage statistics (total calls, tokens used)
    - [ ] Cost per summary (average)
    - [ ] Total monthly costs
    - [ ] Cost trends over time (chart or table)
  - [ ] Query `llm_api_usage` table from Story 7.1
  - [ ] Add "AI Cost Monitoring" link to admin navigation
- [ ] Task 7: Create cost monitoring API endpoint (AC: 4)
  - [ ] Create `GET /api/ai/cost-monitoring` endpoint
  - [ ] Aggregate cost data:
    - [ ] Total API calls (today, this week, this month)
    - [ ] Total tokens used
    - [ ] Total costs
    - [ ] Cost per summary
    - [ ] Cost trends (daily/weekly)
  - [ ] Return cost statistics as JSON
  - [ ] Require authentication
- [ ] Task 8: Implement prompt versioning (AC: 5)
  - [ ] Create `api/services/prompt_manager.py`
  - [ ] Store prompts in database or configuration:
    - [ ] `prompt_id`: UUID
    - [ ] `prompt_type`: VARCHAR(50) ('summarization', 'impact_assessment')
    - [ ] `prompt_text`: TEXT
    - [ ] `version`: INTEGER
    - [ ] `is_active`: BOOLEAN
    - [ ] `created_at`: TIMESTAMP WITH TIME ZONE
  - [ ] Allow A/B testing:
    - [ ] Multiple active prompts for same type
    - [ ] Track which prompt used for each generation
    - [ ] Compare results
- [ ] Task 9: Implement prompt optimization (AC: 5)
  - [ ] Create prompt optimization service
  - [ ] Analyze prompt performance:
    - [ ] Compare quality metrics by prompt version
    - [ ] Compare costs by prompt version
    - [ ] Identify best-performing prompts
  - [ ] Provide recommendations for prompt improvements
- [ ] Task 10: Implement fallback strategies (AC: 6)
  - [ ] Update summarization service:
    - [ ] If API fails, return None (already implemented)
    - [ ] If summary quality is poor (validation fails), flag for review
    - [ ] Allow manual summary override
  - [ ] Add manual summary option in review UI
- [ ] Task 11: Create quality metrics dashboard (AC: 7)
  - [ ] Create `admin-ui/templates/pages/ai/quality-metrics.html` - Quality dashboard
  - [ ] Create web route: `GET /ai/quality-metrics`
  - [ ] Display quality metrics:
    - [ ] Average summary length
    - [ ] User feedback scores (helpful percentage)
    - [ ] Summary generation success rate
    - [ ] Quality trends over time
  - [ ] Add "AI Quality Metrics" link to admin navigation
- [ ] Task 12: Create quality metrics API endpoint (AC: 7)
  - [ ] Create `GET /api/ai/quality-metrics` endpoint
  - [ ] Calculate quality metrics:
    - [ ] Average summary length
    - [ ] User feedback scores
    - [ ] Success rate
    - [ ] Quality trends
  - [ ] Return metrics as JSON
- [ ] Task 13: Create reporting system (AC: 8)
  - [ ] Create `api/services/ai_reporting.py`
  - [ ] Generate quality report:
    - [ ] Summary generation statistics
    - [ ] Quality metrics
    - [ ] User feedback summary
  - [ ] Generate cost report:
    - [ ] Total costs
    - [ ] Cost breakdown by model
    - [ ] Cost trends
  - [ ] Generate usage statistics:
    - [ ] API calls per day/week/month
    - [ ] Tokens used
    - [ ] Models used
  - [ ] Create `GET /api/ai/reports/quality` endpoint
  - [ ] Create `GET /api/ai/reports/cost` endpoint
  - [ ] Create `GET /api/ai/reports/usage` endpoint

## Dev Notes

### Previous Story Insights
- Story 7.1 created OpenRouter API integration and cost tracking
- Story 7.2 created summarization engine
- Story 1.2 created PolicyChange model
- Story 1.5 created User model

### OpenRouter API Integration
[Source: docs/stories/7.1.openrouter-api-integration-and-infrastructure.story.md]

**Cost Tracking:**
- `llm_api_usage` table tracks all API calls
- Includes token counts and costs
- Use for cost monitoring dashboard

### File Locations
[Source: docs/architecture/unified-project-structure.md]

Files to create:
- `api/services/ai_quality_monitoring.py` - Quality monitoring service
- `api/services/prompt_manager.py` - Prompt versioning service
- `api/services/ai_reporting.py` - Reporting service
- `api/models/db/ai_summary_feedback.py` - Feedback model
- `admin-ui/templates/pages/ai/summary-review.html` - Review UI
- `admin-ui/templates/pages/ai/cost-monitoring.html` - Cost dashboard
- `admin-ui/templates/pages/ai/quality-metrics.html` - Quality dashboard
- `alembic/versions/XXX_add_ai_summary_quality_flags.py` - Migration
- `alembic/versions/XXX_add_ai_summary_feedback.py` - Migration

### Coding Standards
[Source: docs/architecture/coding-standards.md]

**Service Layer:**
- Always use service layer for business logic
- Use async/await for all async operations

### Testing Requirements
[Source: docs/architecture/testing-strategy.md]

**Unit Tests:**
- Test quality monitoring logic
- Test prompt versioning
- Test reporting

**Integration Tests:**
- Test review workflow
- Test feedback collection
- Test cost monitoring

## Testing

**Unit Tests:**
- Test quality monitoring logic
- Test prompt versioning
- Test reporting

**Integration Tests:**
- Test review workflow
- Test feedback collection
- Test cost monitoring

**Manual Testing:**
- Review summaries
- Provide feedback
- View cost and quality dashboards
- Generate reports

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-27 | 1.0 | Initial story creation | Bob (Scrum Master) |


